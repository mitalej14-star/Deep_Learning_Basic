{"cells":[{"cell_type":"markdown","metadata":{"id":"1jPLFHhKAWAF"},"source":["# M2177.004300 Deep Learning Assignment #1<br> Part 1-3. Training Multi-Layer RNN (NumPy)"]},{"cell_type":"markdown","metadata":{"id":"67wZJavUAWAI"},"source":["Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jieun Byeon, September 2025\n","\n","**For understanding of this work, please carefully look at given PDF file.**\n","\n","Now, you're going to learn how to implement vanilla RNN using NumPy. <br>\n","\n","\n","![RNN](imgs/rnn.PNG)\n","![the unfolding in time of the computation involved in its forward computation](imgs/unfold.jpg) <br>\n","$x_t$: input at time step $t$<br>\n","$s_t$: hidden state at time step $t$<br>\n","$o_t$: output at time step $t$<br>\n","$W, U, V$: weight for input, hidden state, and output<br>\n","$s_t = tanh(Ux_t + Ws_{t-1})$<br>\n","$o_t = softmax(Vs_t)$<br>"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"MhA9bt5iAWAO","executionInfo":{"status":"ok","timestamp":1759738392255,"user_tz":-540,"elapsed":3,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["import numpy as np\n","\n","seed = 2177\n","np.random.seed(seed)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19161,"status":"ok","timestamp":1759738412587,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"},"user_tz":-540},"id":"rOCkRjq4ag8a","outputId":"2a4feab8-34b1-4d16-987e-3819d27fddeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1759738414244,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"},"user_tz":-540},"id":"VpacqaXzapfU","outputId":"f927987b-8299-424c-c874-c39c06c1eb48"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["%pwd"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495,"status":"ok","timestamp":1759738416316,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"},"user_tz":-540},"id":"gkCeUJY9apmH","outputId":"9459bc20-765d-4658-9b60-176f3594a24c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Assignment1\n"]}],"source":["# Assighment1 경로로 이동\n","%cd /content/drive/MyDrive/Assignment1"]},{"cell_type":"markdown","metadata":{"id":"1vrrpDOeAWAS"},"source":["##  Vanilla RNN: step forward and backward\n","\n","In this section, you will implement step forward and backward passes for **a single timestep** of a vanilla recurrent neural network. Using the code provided as guidance, complete the functions `rnn_step_forward` and `rnn_step_backward`. just write the code in whatever way you find most clear.\n","\n","When you are done, run the following to check your implementations."]},{"cell_type":"markdown","metadata":{"id":"b8OPoOPmAWAU"},"source":["### <a name=\"1\"></a> 1. single timestep forward"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"VgW39rxdAWAV","executionInfo":{"status":"ok","timestamp":1759738417889,"user_tz":-540,"elapsed":4,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["def getdata_rnn_step_forward():\n","\tnp.random.seed(seed)\n","\n","\tN, D, H = 3, 5, 4\n","\tx = np.random.randn(N, D) # current input\n","\tprev_h = np.random.randn(N, H) # previous hidden state\n","\tWx = np.random.randn(D, H) # input -> hidden weights\n","\tWh = np.random.randn(H, H) # hidden -> hidden weights\n","\tb = np.random.randn(H) # biases\n","\n","\texpt_next_h = np.asarray([ # expected next hidden state\n","\t\t[-0.99921173, -0.99967951,  0.39127099, -0.93436299],\n","\t \t[ 0.84348286,  0.99996526, -0.9978802,   0.99996645],\n","\t  [-0.94481752, -0.71940178,  0.99994009, -0.64806562]])\n","\n","\treturn x, prev_h, Wx, Wh, b, expt_next_h\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"EvTZxwvMAWAW","executionInfo":{"status":"ok","timestamp":1759738419138,"user_tz":-540,"elapsed":2,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["#------------------------------------------------------\n","# vanilla rnn step forward\n","#------------------------------------------------------\n","def rnn_step_forward(x, prev_h, Wx, Wh, b):\n","    \"\"\"\n","    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n","    activation function.\n","\n","    The input data has dimension D, the hidden state has dimension H, and we use\n","    a minibatch size of N.\n","\n","    Inputs:\n","    - x: Input data for this timestep, of shape (N, D).\n","    - prev_h: Hidden state from previous timestep, of shape (N, H)\n","    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n","    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n","    - b: Biases of shape (H,)\n","\n","    Returns a tuple of:\n","    - next_h: Next hidden state, of shape (N, H)\n","    - cache: Tuple of values needed for the backward pass.\n","    \"\"\"\n","    next_h, cache = None, None\n","    ##########################################################################\n","    # TODO: Implement a single forward step for the vanilla RNN. Store the next  #\n","    # hidden state and any values you need for the backward pass in the next_h   #\n","    # and cache variables respectively.                                          #\n","    ##########################################################################\n","\n","    a = x.dot(Wx) + prev_h.dot(Wh) + b\n","    next_h = np.tanh(a)\n","    cache = (x, prev_h, Wx, Wh, a)\n","    return next_h, cache\n","\n","    ##########################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##########################################################################\n","    return next_h, cache"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"j5i_dESbAWAX","executionInfo":{"status":"ok","timestamp":1759738421105,"user_tz":-540,"elapsed":2,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1759738422049,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"},"user_tz":-540},"id":"P98bl2ahAWAX","outputId":"b02b5cce-851e-4c1f-fe95-421234ef4135"},"outputs":[{"output_type":"stream","name":"stdout","text":["next_h error:  2.5562554071588464e-09\n"]}],"source":["# implement rnn_step_forward\n","# errors should be less than 1e-8\n","x, prev_h, Wx, Wh, b, expected_next_h = getdata_rnn_step_forward()\n","\n","next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n","\n","print('next_h error: ', rel_error(expected_next_h, next_h))"]},{"cell_type":"markdown","metadata":{"id":"KdRJep8vAWAa"},"source":["### <a name=\"1\"></a> 2. single timestep backward"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jkghMnZTAWAf","executionInfo":{"status":"ok","timestamp":1759738423339,"user_tz":-540,"elapsed":2,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["# function for numerical gradient check\n","def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","    \"\"\"\n","    Evaluate a numeric gradient for a function that accepts a numpy\n","    array and returns a numpy array.\n","    \"\"\"\n","    grad = np.zeros_like(x)\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        ix = it.multi_index\n","\n","        oldval = x[ix]\n","        x[ix] = oldval + h\n","        pos = f(x).copy()\n","        x[ix] = oldval - h\n","        neg = f(x).copy()\n","        x[ix] = oldval\n","\n","        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","        it.iternext()\n","    return grad"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uehQ71-wAWAi","executionInfo":{"status":"ok","timestamp":1759738424583,"user_tz":-540,"elapsed":4,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["# get numerical gradient for rnn_step_backward\n","# ground truth for rnn_step_backward\n","def getdata_rnn_step_backward(x,h,Wx,Wh,b,dnext_h):\n","\n","\tfx  = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\tfh  = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\tfWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\tfWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\tfb  = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\n","\tdx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n","\tdprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n","\tdWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n","\tdWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n","\tdb_num = eval_numerical_gradient_array(fb, b, dnext_h)\n","\n","\treturn dx_num, dprev_h_num, dWx_num, dWh_num, db_num"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"czFc9-2WAWAl","executionInfo":{"status":"ok","timestamp":1759739720591,"user_tz":-540,"elapsed":3,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["#------------------------------------------------------\n","# vanilla rnn step backward\n","#------------------------------------------------------\n","def rnn_step_backward(dnext_h, cache):\n","    \"\"\"\n","    Backward pass for a single timestep of a vanilla RNN.\n","\n","    Inputs:\n","    - dnext_h: Gradient of loss with respect to next hidden state\n","    - cache: Cache object from the forward pass\n","\n","    Returns a tuple of:\n","    - dx: Gradients of input data, of shape (N, D)\n","    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n","    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n","    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n","    - db: Gradients of bias vector, of shape (H,)\n","    \"\"\"\n","    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n","    ##########################################################################\n","    # TODO: Implement the backward pass for a single step of a vanilla RNN.      #\n","    #                                                                            #\n","    # HINT: For the tanh function, you can compute the local derivative in terms #\n","    # of the output value from tanh.                                             #\n","    ##########################################################################\n","\n","    x, prev_h, Wx, Wh, a = cache\n","    dtanh = 1-np.tanh(a)**2\n","    da = dnext_h * dtanh\n","\n","    dx = da.dot(Wx.T)\n","    dprev_h = da.dot(Wh.T)\n","    dWx = x.T.dot(da)\n","    dWh = prev_h.T.dot(da)\n","    db = np.sum(da, axis=0)\n","    ##########################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##########################################################################\n","    return dx, dprev_h, dWx, dWh, db"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"mqUdDiVvAWAn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759738427352,"user_tz":-540,"elapsed":5,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}},"outputId":"55678efb-cb2e-4a94-f07e-37c207b67eae"},"outputs":[{"output_type":"stream","name":"stdout","text":["dx error     :  6.111935337890625e-10\n","dprev_h error:  7.699207487627533e-11\n","dWx error    :  4.917216370202925e-10\n","dWh error    :  9.995079967788294e-11\n","db error     :  3.033863959395676e-11\n"]}],"source":["x, h, Wx, Wh, b, expected_next_h = getdata_rnn_step_forward()\n","out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n","dnext_h = np.random.randn(*out.shape)\n","dx_num, dprev_h_num, dWx_num, dWh_num, db_num = getdata_rnn_step_backward(x,h,Wx,Wh,b,dnext_h)\n","\n","dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n","\n","print('dx error     : ', rel_error(dx_num, dx))\n","print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n","print('dWx error    : ', rel_error(dWx_num, dWx))\n","print('dWh error    : ', rel_error(dWh_num, dWh))\n","print('db error     : ', rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"id":"3nKg3MdLAWAp"},"source":["## Vanilla RNN: forward and backward\n","\n","Combining the single timestep forward and backward passes for vanilla RNN, you will implement a vanilla RNN that process **an entire sequence**. Using the code provided as guidance, complete the functions `rnn_forward` and `rnn_backward`.\n","\n","When you are done, run the following to check your implementations."]},{"cell_type":"markdown","metadata":{"id":"kzU2DvrKAWAp"},"source":["### <a name=\"1\"></a> 3. forward pass for an entire sequence"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"L8w4jG-rAWAq","executionInfo":{"status":"ok","timestamp":1759738911337,"user_tz":-540,"elapsed":3,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["def getdata_rnn_forward():\n","\tnp.random.seed(seed)\n","\n","\tN, D, T, H = 2,3,4,5\n","\tx = np.random.randn(N, T, D)\n","\th0 = np.random.randn(N, H)\n","\tWx = np.random.randn(D, H)\n","\tWh = np.random.randn(H, H)\n","\tb = np.random.randn(H)\n","\n","\texpt_next_h = np.asarray([\n","\t[[ 0.79899136, -0.90076473, -0.69325878, -0.99991011,  0.92991908],\n","   [-0.04474799, -0.99999994, -0.72167573, -0.99942462, -0.98397185],\n","   [ 0.98674954, -0.74668554, -0.30836793, -0.87580427, -0.25076433],\n","   [ 0.99999994,  0.46495278, -0.6291276 ,  0.44811995, -0.91013617]],\n","\n"," \t[[-0.57789921, -0.10875688, -0.99049558, -0.58448393,  0.76942269],\n","   [-0.05646372, -0.99855467, -0.827688  , -0.65262183, -0.98211725],\n","   [ 0.89687939,  0.99998112, -0.99999517,  0.66932722,  0.99952606],\n","   [-0.97608409, -0.64972242, -0.99987169, -0.99747724,  0.99962792]]])\n","\n","\treturn x, h0, Wx, Wh, b, expt_next_h"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Onw2iXUAAWAq","executionInfo":{"status":"ok","timestamp":1759740747242,"user_tz":-540,"elapsed":2,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["#------------------------------------------------------\n","# vanilla rnn forward\n","#------------------------------------------------------\n","def rnn_forward(x, h0, Wx, Wh, b):\n","    \"\"\"\n","    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n","    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n","    size of H, and we work over a minibatch containing N sequences. After running\n","    the RNN forward, we return the hidden states for all timesteps.\n","\n","    Inputs:\n","    - x: Input data for the entire timeseries, of shape (N, T, D).\n","    - h0: Initial hidden state, of shape (N, H)\n","    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n","    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n","    - b: Biases of shape (H,)\n","\n","    Returns a tuple of:\n","    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n","    - cache: Values needed in the backward pass\n","    \"\"\"\n","    h, cache = None, None\n","    ##########################################################################\n","    # TODO: Implement forward pass for a vanilla RNN running on a sequence of    #\n","    # input data. You should use the rnn_step_forward function that you defined  #\n","    # above.                                                                     #\n","    ##########################################################################\n","    N, T, D = x.shape\n","    H = h0.shape[1]\n","\n","    h = np.zeros((N, T, H), dtype=x.dtype)\n","    cache = []\n","    prev_h = h0\n","\n","    for t in range(T):\n","        xt = x[:, t, :]\n","        next_h, step_cache = rnn_step_forward(xt, prev_h, Wx, Wh, b)\n","        h[:, t, :] = next_h\n","        cache.append(step_cache)\n","        prev_h = next_h\n","\n","    return h, cache\n","\n","    ##########################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##########################################################################\n","    return h, cache"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"gFAL9icBAWAr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759740748969,"user_tz":-540,"elapsed":19,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}},"outputId":"50918ba7-c56a-4a4b-e42c-93cf2bf0d0d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["h error:  2.1007953765777348e-08\n"]}],"source":["# implement rnn_forward\n","# errors should be less than 1e-7\n","x,h0,Wx,Wh,b, expected_h = getdata_rnn_forward()\n","\n","h, _ = rnn_forward(x, h0, Wx, Wh, b)\n","\n","print('h error: ', rel_error(expected_h, h))"]},{"cell_type":"markdown","metadata":{"id":"uWhGA33tAWAw"},"source":["### <a name=\"1\"></a> 4. backward pass for an entire sequence"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"9y7aPvz0AWAy","executionInfo":{"status":"ok","timestamp":1759740750182,"user_tz":-540,"elapsed":4,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["def getdata_rnn_backward(x,h0,Wx,Wh,b,dout):\n","\tfx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\tfh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\tfWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\tfWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\tfb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\n","\tdx_num = eval_numerical_gradient_array(fx, x, dout)\n","\tdh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n","\tdWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n","\tdWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n","\tdb_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","\treturn dx_num, dh0_num, dWx_num, dWh_num, db_num"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"P8fJ60DAAWAz","executionInfo":{"status":"ok","timestamp":1759741425252,"user_tz":-540,"elapsed":43,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}}},"outputs":[],"source":["#------------------------------------------------------\n","# vanilla rnn backward\n","#------------------------------------------------------\n","def rnn_backward(dh, cache):\n","    \"\"\"\n","    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n","\n","    Inputs:\n","    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n","\n","    Returns a tuple of:\n","    - dx: Gradient of inputs, of shape (N, T, D)\n","    - dh0: Gradient of initial hidden state, of shape (N, H)\n","    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n","    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n","    - db: Gradient of biases, of shape (H,)\n","    \"\"\"\n","    dx, dh0, dWx, dWh, db = None, None, None, None, None\n","\n","    ##########################################################################\n","    # TODO: Implement the backward pass for a vanilla RNN running an entire      #\n","    # sequence of data. You should use the rnn_step_backward function that you   #\n","    # defined above.                                                             #\n","    ##########################################################################\n","    T = len(cache)\n","    x0, prev_h0, Wx0, Wh0, a0 = cache[0]\n","    N, D = x0.shape\n","    H = prev_h0.shape[1]\n","\n","    dx  = np.zeros((N, T, D), dtype=x0.dtype)\n","    dWx = np.zeros_like(Wx0)\n","    dWh = np.zeros_like(Wh0)\n","    db  = np.zeros((H,), dtype=x0.dtype)\n","\n","    dprev_h = np.zeros((N, H), dtype=x0.dtype)\n","\n","    for t in reversed(range(T)):\n","        dnext_h = dh[:, t, :] + dprev_h\n","        dx_t, dprev_h, dWx_t, dWh_t, db_t = rnn_step_backward(dnext_h, cache[t])\n","\n","        dx[:, t, :] += dx_t\n","        dWx += dWx_t\n","        dWh += dWh_t\n","        db  += db_t\n","\n","    dh0 = dprev_h\n","    return dx, dh0, dWx, dWh, db\n","\n","    ##########################################################################\n","    #                               END OF YOUR CODE                             #\n","    ##########################################################################\n","    return dx, dh0, dWx, dWh, db"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ugfidgS0AWA0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759741426285,"user_tz":-540,"elapsed":20,"user":{"displayName":"­이은지 / 학생 / 기계공학부","userId":"15027322168102395533"}},"outputId":"a74da5d3-51ab-4059-c010-e9d2f68bac37"},"outputs":[{"output_type":"stream","name":"stdout","text":["dx error:  1.9080063186473363e-08\n","dh0 error:  7.196103315383523e-10\n","dWx error:  1.3418004368964638e-10\n","dWh error:  5.665377918363601e-09\n","db error:  8.441613976110239e-10\n"]}],"source":["x,h0,Wx,Wh,b, expected_h = getdata_rnn_forward()\n","out, cache = rnn_forward(x, h0, Wx, Wh, b)\n","dout = np.random.randn(*out.shape)\n","dx_num, dh0_num, dWx_num, dWh_num, db_num = getdata_rnn_backward(x,h0,Wx,Wh,b,dout)\n","\n","dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dh0 error: ', rel_error(dh0_num, dh0))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"deep-learning-25","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}