{"cells":[{"cell_type":"markdown","metadata":{"id":"I0efNL5YPjG9"},"source":["# M2177.004300 Deep Learning Assignment #2<br> Part 2. Training Vision Transformers (PyTorch)"]},{"cell_type":"markdown","metadata":{"id":"rEXEEGg2PjG-"},"source":["Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Youngwoo Kimh, October 2025"]},{"cell_type":"markdown","metadata":{"id":"5fFabg-6PjG_"},"source":["**For understanding of this work, please carefully look at given PDF file.**\n","\n","Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **PyTorch**. <br>\n","In this notebook, you will learn to understand and build the basic components of Vision Tranformer(ViT). Then, you will try to classify images in the FashionMNIST datatset and explore the effects of different components of ViTs.\n","<br>\n","There are **2 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n","\n","**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n","\n","### Submitting your work:\n","<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.\n","\n","### Some helpful tutorials and references for assignment #2-2:\n","- [1] Pytorch official documentation. [[link]](https://pytorch.org/docs/stable/index.html)\n","- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n","- [3] Alexey Dosovitskiy et al., \"An Image is Worth 16 x 16 Words: Transformers for Image Recognition at Scale\", ICLR 2021. [[pdf]](https://arxiv.org/pdf/2010.11929.pdf)"]},{"cell_type":"markdown","metadata":{"id":"A0F9VZHXPjG_"},"source":["## 1. Building Vision Transformer\n","Here, you will build the basic components of Vision Transformer(ViT). <br>\n","\n","![Vision Transformer](imgs/ViT.png)\n","\n","Using the explanation and code provided as guidance, <br>\n","Define each component of ViT. <br>\n","\n","\n","#### ViT architecture:\n","* ViT model consists with input patch embedding, positional embeddings, transformer encoder, etc.\n","* Patch embedding\n","* Positional embeddings\n","* Transformer encoder with\n","    * Attention module\n","    * MLP module"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37736,"status":"ok","timestamp":1761564447255,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"jr1qebKzPjHA","outputId":"d4b343f8-9760-4e73-cbed-7f2622cdc830"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1761564447260,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"U6Y00M_FPjHA","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"ddfb04c3-da5b-4600-e0ce-9de4c780932e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["%pwd # 현재 경로 확인"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":859,"status":"ok","timestamp":1761564448122,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"wdItYtfPPjHB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e7ed28e-1e00-400e-d6ef-a44b62962b62"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Assignment2\n"]}],"source":["# Assighment1 경로로 이동\n","%cd /content/drive/MyDrive/Assignment2"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9181,"status":"ok","timestamp":1761564457304,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"3MtKkLsdPjHB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e657dc7d-50c9-42d7-d6c0-d724f824e21e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7bb2a7f8fe30>"]},"metadata":{},"execution_count":4}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from torch.optim import AdamW\n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","g = torch.Generator()\n","g.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"IblavwCUPjHC"},"source":["##### Patch Embed\n","\n","**Initialization**: When you create an instance of the PatchEmbedding class, you specify the image_size, patch_size, and in_channels. image_size is the height and width of the input image, patch_size is the size of each patch, and in_channels is the number of input image channels (e.g., 3 for RGB images).\n","\n","**Convolutional Projection**: Inside the PatchEmbedding class, a 2D convolutional layer (nn.Conv2d) is used to perform a patch-based projection. This convolutional layer has a kernel size of patch_size, which defines the size of each patch, and a stride of patch_size, which ensures that patches do not overlap. The convolutional layer effectively extracts image patches.\n","\n","**Reshaping**: After the convolutional projection, the output tensor is reshaped using view. It is transformed from a 4D tensor with dimensions (batch_size, in_channels, H, W) to a 3D tensor with dimensions (batch_size, num_patches, patch_dim). num_patches is the total number of non-overlapping patches in the image, and patch_dim is the number of output channels from the convolutional layer."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1761564457326,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"bPQaGZjrPjHC"},"outputs":[],"source":["class PatchEmbed(nn.Module):\n","    \"\"\"ConvStem Patch Embedding for small grayscale images (keeps 8x8 tokens at 32x32 input).\"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        #stem\n","        c1 = embed_dim // 4\n","        c2 = embed_dim // 2\n","        self.stem = nn.Sequential(\n","            nn.Conv2d(in_chans, c1, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(c1), nn.GELU(),\n","            nn.Conv2d(c1, c2, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(c2), nn.GELU(),\n","            nn.Conv2d(c2, embed_dim, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(embed_dim), nn.GELU(),\n","        )\n","\n","        self.proj = nn.Conv2d(embed_dim, embed_dim, kernel_size=2, stride=2)\n","\n","        out_side = img_size // 4\n","        self.grid_size = (out_side, out_side)\n","        self.num_patches = out_side * out_side\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","    def forward(self, x):\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        x = self.stem(x)\n","        x = self.proj(x)\n","        x = x.flatten(2).transpose(1, 2)\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"9HpkMubkPjHD"},"source":["##### Attention\n","\n","**Initialization**\n","* dim: The input dimension of the sequence. This is the dimensionality of the queries, keys, and values.\n","* num_heads: The number of attention heads to use. Multi-head attention allows the model to focus on different parts of the input simultaneously.\n","\n","**Linear Projections (qkv and proj)**: The qkv linear layer takes the input sequence and projects it into three parts: queries (q), keys (k), and values (v). The output of this layer has a shape of (batch_size, sequence_length, 3 * dim).\n","\n","**Forward Pass (forward method)**: In the forward pass, the input tensor x is processed through the attention mechanism. Here's what happens:<br>\n","* The linear projection qkv is applied to x, producing a tensor of shape (batch_size, sequence_length, 3 * dim).|\n","* This tensor is reshaped to have dimensions (batch_size, sequence_length, 3, num_heads, head_dim). The permute operation rearranges the dimensions to (3, batch_size, num_heads, sequence_length, head_dim), making it suitable for multi-head attention.\n","* The three parts, q, k, and v, are extracted from the reshaped tensor.\n","* The attention scores are computed by taking the dot product of queries q and keys k. The result is scaled by self.scale.\n","* The attention scores are passed through a softmax activation along the last dimension (sequence_length), producing attention weights.\n","* The weighted sum of values v is computed using the attention weights.\n","* The result is transposed and reshaped to its original shape, and then passed through the proj linear layer.\n","* The final output is returned."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1761564457329,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"_SWA8e5nPjHD"},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3)\n","        self.proj = nn.Linear(dim, dim)\n","\n","        self.attn_drop = nn.Dropout(0.08) #dropout 추가\n","        self.proj_drop = nn.Dropout(0.08)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        qkv = self.qkv(x)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x # output dimension must be: (batch size, number of patches, embed_dim)"]},{"cell_type":"markdown","metadata":{"id":"9uydiF27PjHD"},"source":["##### MLP\n","\n","The MLP module must consist of three layers:\n","* fully conncted layer 1\n","* activation layer\n","* fully conncted layer 2"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1761564457375,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"UGXEULJJPjHE"},"outputs":[],"source":["class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(0.1) #dropout 추가\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","\n","    def forward(self, x):\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","        return x # output dimension must be: (batch size, number of patches, out_features)"]},{"cell_type":"markdown","metadata":{"id":"U1fgRzyRPjHE"},"source":["##### Transformer Block\n","The transformer block contains the attention module and MLP module which have residual connections.\n","Refer to the following image and build the forward pass.\n","\n","![Transformer Block](imgs/TransformerBlock.png)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1761564457376,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"EPfrOhibPjHE"},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio=4., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(dim, num_heads=num_heads)\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n","                       act_layer=act_layer)\n","\n","    def forward(self, x):\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        x = x + self.attn(self.norm1(x))\n","        x = x + self.mlp(self.norm2(x))\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"Ii1Mf-Z-PjHE"},"source":["##### Vision Transformer\n","\n","Using all the components that you built above, **complete** the vision transformer class."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1761564457376,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"IYh9e9ATPjHE"},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer \"\"\"\n","\n","    def __init__(self, img_size=28, patch_size=4, in_chans=1, num_classes=10, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., norm_layer=nn.LayerNorm, ):\n","        super().__init__()\n","        self.num_features = self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.depth = depth\n","\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        num_patches = self.patch_embed.num_patches\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        # similarly to cls_token, define a learnable positional embedding that matches the patchified input token size.\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,  norm_layer=norm_layer)\n","            for i in range(depth)])\n","        self.norm = norm_layer(embed_dim)\n","\n","        # Classifier head\n","        self.head = nn.Linear(\n","            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward(self, x):\n","        ##############################################################################\n","        #                           IMPLEMENT YOUR CODE                              #\n","        ##############################################################################\n","        B = x.shape[0]\n","\n","        # Patch Embedding\n","        x = self.patch_embed(x)\n","\n","        # Concatenate class tokens to patch embedding\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        # Add positional embedding to patches\n","        x = x + self.pos_embed\n","\n","        # Forward through encoder blocks\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","\n","        # Use class token for classification\n","        cls_token_final = x[:, 0]\n","\n","        # Classifier head\n","        x = self.head(cls_token_final)\n","        ##############################################################################\n","        #                              END YOUR CODE                                 #\n","        ##############################################################################\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"9Id9RsqlPjHF"},"source":["## 2. Training a small ViT model on FashionMNIST dataset.\n","\n","Define and Train a vision transformer on FashionMNIST dataset. **(You must reach above 85% for full points.)** <br>\n","Train with at least 5 different hyperparameter settings varying the following ViT hyperparameters.\n","Report the setting for the best performance.\n","\n","#### ViT hyperparameters:\n","* patch_size\n","* embed_dim\n","* depth\n","* num_heads\n","* mlp_ratio\n","* etc.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1761564457378,"user":{"displayName":"이은지","userId":"07778628580437269752"},"user_tz":-540},"id":"pLpdB47LPjHF"},"outputs":[],"source":["import numpy as np\n","\n","from tqdm import tqdm, trange\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","\n","from torchvision.transforms import ToTensor\n","from torchvision.datasets.mnist import FashionMNIST\n","\n","import math\n","from torch.optim import AdamW #최적화\n","from torch.optim.lr_scheduler import LambdaLR #스케쥴러\n","\n","import copy"]},{"cell_type":"markdown","metadata":{"id":"e9qpsmToc3S_"},"source":[]},{"cell_type":"code","source":["import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"2KbOxp6B5XDs","executionInfo":{"status":"ok","timestamp":1761569034571,"user_tz":-540,"elapsed":39,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"id":"lZWwg6Z8PjHF","executionInfo":{"status":"ok","timestamp":1761570256865,"user_tz":-540,"elapsed":1221663,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2af9ff2e-69ed-4a78-932b-2b7453593719"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda (Tesla T4)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[01/20] loss=0.937\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[02/20] loss=0.669\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[03/20] loss=0.593\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[04/20] loss=0.557\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[05/20] loss=0.539\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[06/20] loss=0.550\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[07/20] loss=0.528\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[08/20] loss=0.494\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[09/20] loss=0.451\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[10/20] loss=0.426\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[11/20] loss=0.396\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[12/20] loss=0.381\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[13/20] loss=0.356\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[14/20] loss=0.336\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[15/20] loss=0.322\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[16/20] loss=0.301\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[17/20] loss=0.288\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[18/20] loss=0.275\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[19/20] loss=0.265\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["[20/20] loss=0.258\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|██████████| 20/20 [00:03<00:00,  5.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test loss: 0.25\n","Test accuracy: 91.22%\n","Saved Trained Model.\n"]}],"source":["def Train():\n","    img_size   = 28\n","    patch_size = 4\n","    embed_dim  = 192\n","    depth      = 12\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 3e-3\n","\n","    N_EPOCHS   = 20\n","    LR_MAX     = 3e-3\n","    WD         = 5e-2\n","    WARMUP_PCT = 0.30\n","    DIV        = 25.0\n","    FINAL_DIV  = 100.0\n","    CLIP_NORM  = 1.0\n","\n","    # 데이터셋/증강\n","    train_transform = transforms.Compose([\n","        transforms.RandomCrop(28, padding=4),\n","        transforms.RandomHorizontalFlip(0.5),\n","        transforms.RandomRotation(8),\n","        transforms.ToTensor(),\n","        transforms.RandomErasing(p=0.10, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n","    ])\n","    test_transform = transforms.ToTensor()\n","\n","    train_set = FashionMNIST(root='./data', train=True,  download=True, transform=train_transform)\n","    test_set  = FashionMNIST(root='./data', train=False, download=True, transform=test_transform)\n","\n","    train_loader = DataLoader(train_set, shuffle=True,  batch_size=192,\n","                              num_workers=2, pin_memory=True, persistent_workers=True)\n","    test_loader  = DataLoader(test_set,  shuffle=False, batch_size=512,\n","                              num_workers=2, pin_memory=True, persistent_workers=True)\n","\n","    torch.backends.cudnn.benchmark = True\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n","\n","    model = VisionTransformer(patch_size=patch_size, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio).to(device)\n","    model_path = './vit.pth'\n","\n","    #최적화/스케줄러\n","    optimizer = AdamW(model.parameters(), lr=LR_MAX, weight_decay=WD, betas=(0.9, 0.999))\n","\n","    steps_per_epoch = len(train_loader)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","        optimizer, max_lr=LR_MAX,\n","        epochs=N_EPOCHS, steps_per_epoch=steps_per_epoch,\n","        pct_start=WARMUP_PCT, anneal_strategy='cos',\n","        div_factor=DIV, final_div_factor=FINAL_DIV\n","    )\n","\n","    criterion = CrossEntropyLoss()\n","\n","    #Train loop\n","    for epoch in range(N_EPOCHS):\n","        model.train()\n","        train_loss = 0.0\n","\n","        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\", leave=False, mininterval=2.0):\n","            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            y_hat = model(x)\n","            loss  = criterion(y_hat, y)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n","            optimizer.step()\n","\n","            scheduler.step()\n","            train_loss += loss.detach().cpu().item() / len(train_loader)\n","\n","        print(f\"[{epoch+1:02d}/{N_EPOCHS}] loss={train_loss:.3f}\")\n","\n","    # Test loop\n","    with torch.no_grad():\n","        model.eval()\n","        correct, total = 0, 0\n","        test_loss = 0.0\n","        for batch in tqdm(test_loader, desc=\"Testing\"):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            loss = criterion(y_hat, y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","\n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            total += len(x)\n","        print(f\"Test loss: {test_loss:.2f}\")\n","        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n","\n","    torch.save(model.state_dict(), model_path)\n","    print('Saved Trained Model.')\n","\n","Train()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"V23Pl1UyPjHF","executionInfo":{"status":"ok","timestamp":1761565307661,"user_tz":-540,"elapsed":41,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["# For Google Colab\n","# Uncomment the next line and run. The compressed file will be in your assignment directory.\n","!bash CollectSubmission.sh [Your Student ID]"]},{"cell_type":"markdown","metadata":{"id":"j9jJsKI8PjHG"},"source":["### Describe what you did and discovered here\n","In this cell you should write all the settings tried and performances you obtained. Report what you did and what you discovered from the trials.\n","You can write in Korean"]},{"cell_type":"markdown","metadata":{"id":"H_KJjy0gPjHG"},"source":["1. 데이터 증강 : 일반화 도움\n","2. AdamW 사용\n","3. 스케줄러 사용\n","4. embedding에서 conv 로 stem\n","5. dropout 사용\n","\n","#Hyper parameter(공통)\n","\n","    N_EPOCHS   = 20\n","    WD         = 5e-2\n","    WARMUP_PCT = 0.30\n","    DIV        = 25.0\n","    FINAL_DIV  = 100.0\n","    CLIP_NORM  = 1.0\n","\n","#Base (Acc = 91.35%)\n","    patch_size = 4\n","    embed_dim  = 192\n","    depth      = 10\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 3e-3\n","\n","#Variant 1 (Acc = 91.33%)\n","    patch_size = 4\n","    embed_dim  = 192\n","    depth      = 10\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 2e-3\n","\n","LR을 낮춰 수렴 속도 비교. 거의 차이 없음\n","\n","#Variant 2 (Acc = 91.98%)\n","    patch_size = 4\n","    embed_dim  = 128\n","    depth      = 10\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 3e-3\n","\n","embed_dim을 128로 줄임. 성능 약간 향상 (gpu 한계로 다른 계정에서 실험)\n","\n","#Variant 3 (Acc = 90.87%)\n","    patch_size = 4\n","    embed_dim  = 256\n","    depth      = 10\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 3e-3\n","\n","embed_dim을 256으로 늘림. train loss는 계속 하락했으나 정확도는 떨어짐.(gpu 한계로 다른 계정에서 실험)\n","\n","#Variant 4 (Acc = 91.14%)\n","    patch_size = 4\n","    embed_dim  = 192\n","    depth      = 8\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 3e-3\n","\n","depth를 8로 줄. base와 차이 미미(gpu 한계로 다른 계정에서 실험)\n","\n","#Variant 5 (Acc = 91.22%)\n","    patch_size = 4\n","    embed_dim  = 192\n","    depth      = 12\n","    num_heads  = 8\n","    mlp_ratio  = 3.5\n","    LR_MAX     = 3e-3\n","\n","depth =12 로 늘림. 8, 10, 12 다 미미한 성능 차이를 보였다.\n","\n","#실험 결론\n","embed dim은 늘리는것보다 줄이는 것이 좋다\n","depth는 거의 차이가 없으나 10이 12, 8 보다 좋은 듯 하다._Tell us here_"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":0}