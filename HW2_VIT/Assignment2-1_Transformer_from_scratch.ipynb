{"cells":[{"cell_type":"markdown","metadata":{"id":"yW76vaHnhvH4"},"source":["# M2177.004300 Deep Learning Assignment #2<br> Part 1. Transformer from scratch (PyTorch)"]},{"cell_type":"markdown","metadata":{"id":"YGVhA7MAhvIB"},"source":["Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Youngwoo Kimh, October 2025"]},{"cell_type":"markdown","metadata":{"id":"RNIV7kmEhvIC"},"source":["**For understanding of this work, please carefully\n","look at given PDF file.**\n","\n","In this notebook, you will learn to implement a transformer model from scratch. By doing so, you will understand the nuts and bolts of Transformers more clearly at a code level.\n","<br>\n","There are **5 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n","\n","**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n","\n","### Submitting your work:\n","<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n","\n","### Some helpful tutorials and references for assignment #2-1:\n","- [1] Original Transformer paper(Vaswani et al., 2017). [[link]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n","- [2] Helpful instructions about how Transformer works. [[link]](https://github.com/jadore801120/attention-is-all-you-need-pytorch)     "]},{"cell_type":"markdown","metadata":{"tags":[],"id":"JJcZ8TgShvIE"},"source":["### Check virtual env and import packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99Mj-5lRhvIG","executionInfo":{"status":"ok","timestamp":1761571954951,"user_tz":-540,"elapsed":25414,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"outputId":"1c499281-ea4e-469d-a8b0-d74986fcc10d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QJu7ioYPhvIK","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1761571954954,"user_tz":-540,"elapsed":9,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"outputId":"9b3f97c4-c22c-4c47-cea0-deaee9c8ddb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["%pwd # 현재 경로 확인"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uMaQ0OXVhvIM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761571955411,"user_tz":-540,"elapsed":456,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"outputId":"9890c35f-2b66-47c7-b93c-a86be528046d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Assignment2\n"]}],"source":["# Assighment1 경로로 이동\n","%cd /content/drive/MyDrive/Assignment2"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"J25dyXgChvIN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761571959690,"user_tz":-540,"elapsed":4269,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"outputId":"8f83697e-aa96-4943-d6f4-86e8c4a53824"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7a9b3287ef50>"]},"metadata":{},"execution_count":4}],"source":["import os\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","\n","\n","if torch.cuda.is_available() is True:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","g = torch.Generator()\n","g.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"Zlz_GJ2IhvIP"},"source":["## Overview of the model"]},{"cell_type":"markdown","metadata":{"id":"d9DwDCmuhvIQ"},"source":["![encoder](./imgs/Model_small.png)"]},{"cell_type":"markdown","metadata":{"id":"7D4Bkn6qhvIQ"},"source":["## 1. Positional Encoding"]},{"cell_type":"markdown","metadata":{"id":"GkfZeCJXhvIR"},"source":["According to the original paper on Transformer, positional encoding is constructed by using sine functions to even dimensions and cosine functions to odd dimensions.\n","\n","\\begin{align*}\n","    PE_{(pos,2i)} = sin(pos / 10000^{2i/dim}) \\\\\n","    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/dim})\n","\\end{align*}"]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true,"id":"_jAY6LWNhvIR","executionInfo":{"status":"ok","timestamp":1761571959698,"user_tz":-540,"elapsed":10,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dim, seq_len_max):\n","        super(PositionalEncoding, self).__init__()\n","        PE = torch.zeros(seq_len_max, dim)\n","        ######################### TO DO #########################\n","        pos = torch.arange(0, seq_len_max, dtype = torch.float).unsqueeze(1)\n","        expo = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) /dim))\n","\n","        PE[:, 0::2] = torch.sin(pos * expo)\n","        PE[:, 1::2] = torch.cos(pos * expo)\n","\n","        ######################### TO DO #########################\n","\n","        ######################### DO NOT CHANGE #########################\n","        # Positional Encoding is not learnable parameters.\n","        self.register_buffer('PE', PE.unsqueeze(0))\n","        ######################### DO NOT CHANGE #########################\n","\n","    def forward(self, X):\n","        return X + self.PE[:, :X.size(1)]"]},{"cell_type":"markdown","metadata":{"id":"jtT83TMChvIS"},"source":["## 2. Multi-head attention"]},{"cell_type":"markdown","metadata":{"id":"NvnvqSKJhvIS"},"source":["![multi_head_attention](./imgs/Attention.png)"]},{"cell_type":"markdown","metadata":{"id":"QAH7MWwthvIT"},"source":["In this section, we will implement MultiHeadAttention Class.  \n","The parameters of MultiHeadAttention class is defined as follows.\n","Note that according to the definition of multi-head attention, the dimension of the model is equal to the product\n","of the word dimension and the number of heads\n","\n","$dim$:  dimension of the model  \n","$dim$ = dimension for a each word * $head\\_num$  \n","$seq\\_len$:  length of the input sequence\n","\n","This module will get batched sequences x and return multi-head attention ouput.\n","\n","X size:  $(batch\\_num, seq\\_len, dim)$  \n","mask: Tensor to indicate the words involved in score calculation  \n","output size:  $(batch\\_num, seq\\_len, dim)$\n","\n","$W_q$ = linear transformation for query  \n","$W_k$ = linear transformation for key    \n","$W_v$ = linear transformation for value  \n","$W_o$ = linear transformation for concatenated heads\n","\n","The model operates according to the following equation.  \n","It should select the values that will participate in score calculation based on the received mask.\n","\n","$Q = X * W_q$  \n","$K = X * W_k$  \n","$V = X * W_v$  \n","\n","$scores = \\frac{QK^T}{\\sqrt{word\\_dim}}$  \n","$masked\\_scores = mask(\\frac{QK^T}{\\sqrt{word\\_dim}})$  \n","$probs = softmax(masked\\_scores)$  \n","$heads = probsV$  \n","$output = heads * W_o$  \n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2iWYMrI7hvIT","executionInfo":{"status":"ok","timestamp":1761571959699,"user_tz":-540,"elapsed":9,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, dim, head_num):\n","        super(MultiHeadAttention, self).__init__()\n","\n","        self.dim = dim\n","        self.head_num = head_num\n","        self.word_dim = dim // head_num\n","\n","        ######################### TO DO #########################\n","        self.W_q = nn.Linear(dim, dim)\n","        self.W_k = nn.Linear(dim, dim)\n","        self.W_v = nn.Linear(dim, dim)\n","        self.W_o = nn.Linear(dim, dim)\n","        ######################### TO DO #########################\n","\n","    def scaled_dot_product(self, Q, K, V, mask=None):\n","        ######################### TO DO #########################\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.word_dim)\n","        if mask is not None :\n","          masked_scores = scores.masked_fill(mask ==0, float('-inf'))\n","        else :\n","          masked_scores = scores\n","        probs = torch.nn.functional.softmax(masked_scores, dim = -1)\n","        heads = torch.matmul(probs, V)\n","\n","        ######################### TO DO #########################\n","        return heads\n","\n","    def split(self, X):\n","        batch_num, seq_len, dim = X.size()\n","        return X.view(batch_num, seq_len, self.head_num, self.word_dim).transpose(1, 2)\n","\n","    def combine(self, X):\n","        batch_num, _, seq_len, _ = X.size()\n","        return X.transpose(1, 2).contiguous().view(batch_num, seq_len, self.dim)\n","\n","    def forward(self, X_Q, X_K, X_V, mask=None):\n","        Q = self.split(self.W_q(X_Q))\n","        K = self.split(self.W_k(X_K))\n","        V = self.split(self.W_v(X_V))\n","\n","        heads = self.scaled_dot_product(Q, K, V, mask)\n","        output = self.W_o(self.combine(heads))\n","        return output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pTn7AaW5hvIU"},"source":["## 3. Encoder"]},{"cell_type":"markdown","metadata":{"id":"6Sqy3xi5hvIU"},"source":["Implement EncoderLayer class using **one MultiHeadAttention layer, one FNN layer and two normalization layer**.  \n","**Please apply dropout right after passing through multi-head attention and FFN layer.**\n","\n","**HINT**  \n","**1. Normalization is a LayerNorm.**  \n","**2. LayerNorm layers have learnable parameters. Therefore, you should use two normalization layers.**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3bFPqyn3hvIU","executionInfo":{"status":"ok","timestamp":1761571959701,"user_tz":-540,"elapsed":7,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["class FFN(nn.Module):\n","    def __init__(self, dim, FFN_dim):\n","        super(FFN, self).__init__()\n","        self.FFN_layer = nn.Sequential(nn.Linear(dim, FFN_dim),\n","                                       nn.ReLU(),\n","                                       nn.Linear(FFN_dim, dim))\n","    def forward(self, X):\n","        return self.FFN_layer(X)\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, dim, head_num, FFN_dim, dropout):\n","        super(EncoderLayer, self).__init__()\n","        ######################### TO DO #########################\n","        self.self_attn = MultiHeadAttention(dim, head_num)\n","        self.ffn = FFN(dim, FFN_dim)\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.norm2 = nn.LayerNorm(dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        ######################### TO DO #########################\n","\n","    def forward(self, X, mask):\n","        ######################### TO DO #########################\n","        attn_X = self.self_attn(X, X, X, mask)\n","        X = X + self.dropout(attn_X)\n","        X = self.norm1(X)\n","\n","        ffn_X = self.ffn(X)\n","        X = X + self.dropout(ffn_X)\n","        X = self.norm2(X)\n","        output = X\n","        ######################### TO DO #########################\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"sf7DBZ3RhvIV"},"source":["## 4. Decoder"]},{"cell_type":"markdown","metadata":{"id":"B9FhZ0PZhvIV"},"source":["Implement DecoderLayer class using **two MultiHeadAttention layers(self-attention and cross-attention), one FNN layer and three normalization layers.**\n","**Please apply dropout right after passing through two multi-head attention layers and FFN layer.**\n","\n","**HINT**  \n","**1. Normalization is a LayerNorm.**  \n","**2. LayerNorm layers have learnable parameters. Therefore, you should use three normalization layers.**  \n","**3. The first multi-head attention layer is a self attention layer, and the second attention layer is a cross attention layer. Choose the mask carefully.**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"QRvQJ0nHhvIW","executionInfo":{"status":"ok","timestamp":1761571959702,"user_tz":-540,"elapsed":6,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, dim, head_num, FFN_dim, dropout):\n","        super(DecoderLayer, self).__init__()\n","        ######################### TO DO #########################\n","\n","        self.self_attn = MultiHeadAttention(dim, head_num)\n","        self.cross_attn = MultiHeadAttention(dim, head_num)\n","        self.ffn = FFN(dim, FFN_dim)\n","\n","        self.norm = nn.LayerNorm(dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        ######################### TO DO #########################\n","\n","    def forward(self, X, enc_output, cross_attn_mask, self_attn_mask):\n","        ######################### TO DO #########################\n","        self_attn_X = self.self_attn(X, X, X, self_attn_mask)\n","        X = X + self.dropout(self_attn_X)\n","        X = self.norm(X)\n","\n","        cross_attn_X = self.cross_attn(X, enc_output, enc_output, cross_attn_mask)\n","        X = X + self.dropout(cross_attn_X)\n","        X = self.norm(X)\n","\n","        ffn_X = self.ffn(X)\n","        X = X + self.dropout(ffn_X)\n","        X = self.norm(X)\n","        output = X\n","        ######################### TO DO #########################\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"0ABxjsKhhvIW"},"source":["## 5. Prepare sample data and Run model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"AcFZckUGhvIW","executionInfo":{"status":"ok","timestamp":1761571959703,"user_tz":-540,"elapsed":6,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, input_lib_size, output_lib_size, dim, head_num, layer_num, \\\n","                 FFN_dim, seq_len_max, dropout):\n","        super(Transformer, self).__init__()\n","        self.enc_embeds = nn.Embedding(input_lib_size, dim)\n","        self.dec_embeds = nn.Embedding(output_lib_size, dim)\n","        self.pe = PositionalEncoding(dim, seq_len_max)\n","\n","        self.encoder = nn.ModuleList([EncoderLayer(dim, head_num, FFN_dim, dropout) \\\n","                                             for _ in range(layer_num)])\n","        self.decoder = nn.ModuleList([DecoderLayer(dim, head_num, FFN_dim, dropout) \\\n","                                             for _ in range(layer_num)])\n","        self.Linear = nn.Linear(dim, output_lib_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def generate_mask(self, src, tgt):\n","        self_attn_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","        cross_attn_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n","        seq_length = tgt.size(1)\n","        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n","        nopeak_mask = nopeak_mask.to(device)\n","        cross_attn_mask = cross_attn_mask & nopeak_mask\n","        return self_attn_mask, cross_attn_mask\n","\n","    def forward(self, src, tgt):\n","        self_attn_mask, cross_attn_mask = self.generate_mask(src, tgt)\n","        src_embeds = self.dropout(self.pe(self.enc_embeds(src)))\n","        tgt_embeds = self.dropout(self.pe(self.dec_embeds(tgt)))\n","\n","        enc_output = src_embeds\n","        for enc_layer in self.encoder:\n","            enc_output = enc_layer(enc_output, self_attn_mask)\n","\n","        dec_output = tgt_embeds\n","        for dec_layer in self.decoder:\n","            dec_output = dec_layer(dec_output, enc_output, self_attn_mask, cross_attn_mask)\n","\n","        output = self.Linear(dec_output)\n","        return output"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7FER4tmWhvIX","executionInfo":{"status":"ok","timestamp":1761571960282,"user_tz":-540,"elapsed":571,"user":{"displayName":"이은지","userId":"07778628580437269752"}}},"outputs":[],"source":["input_lib_size = 5000\n","output_lib_size = 5000\n","dim = 512\n","head_num = 4\n","layer_num = 3\n","FFN_dim = 2048\n","seq_len_max = 100\n","dropout = 0.1\n","\n","transformer = Transformer(input_lib_size, output_lib_size, dim, head_num, layer_num, \\\n","                          FFN_dim, seq_len_max, dropout)\n","transformer = transformer.to(device)\n","\n","# Generate random sample data\n","src_data = torch.randint(1, input_lib_size, (64, seq_len_max)).to(device)\n","tgt_data = torch.randint(1, output_lib_size, (64, seq_len_max)).to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":true,"id":"_NLyhF3QhvIY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761571994176,"user_tz":-540,"elapsed":33886,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"outputId":"f118e5c3-61c9-43b4-bc40-783dd8617812"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 8.677824020385742\n","Epoch: 2, Loss: 8.580459594726562\n","Epoch: 3, Loss: 8.498089790344238\n","Epoch: 4, Loss: 8.441888809204102\n","Epoch: 5, Loss: 8.392080307006836\n","Epoch: 6, Loss: 8.345414161682129\n","Epoch: 7, Loss: 8.300348281860352\n","Epoch: 8, Loss: 8.248345375061035\n","Epoch: 9, Loss: 8.195426940917969\n","Epoch: 10, Loss: 8.138192176818848\n","Epoch: 11, Loss: 8.078092575073242\n","Epoch: 12, Loss: 8.022431373596191\n","Epoch: 13, Loss: 7.9623589515686035\n","Epoch: 14, Loss: 7.908764362335205\n","Epoch: 15, Loss: 7.848241329193115\n","Epoch: 16, Loss: 7.788707256317139\n","Epoch: 17, Loss: 7.727346897125244\n","Epoch: 18, Loss: 7.668664932250977\n","Epoch: 19, Loss: 7.606161117553711\n","Epoch: 20, Loss: 7.549562931060791\n","Epoch: 21, Loss: 7.487461090087891\n","Epoch: 22, Loss: 7.426892280578613\n","Epoch: 23, Loss: 7.365617752075195\n","Epoch: 24, Loss: 7.303801536560059\n","Epoch: 25, Loss: 7.235662937164307\n","Epoch: 26, Loss: 7.177582740783691\n","Epoch: 27, Loss: 7.115059852600098\n","Epoch: 28, Loss: 7.051268100738525\n","Epoch: 29, Loss: 6.992549419403076\n","Epoch: 30, Loss: 6.925839424133301\n","Epoch: 31, Loss: 6.8660664558410645\n","Epoch: 32, Loss: 6.7994890213012695\n","Epoch: 33, Loss: 6.732519149780273\n","Epoch: 34, Loss: 6.667792320251465\n","Epoch: 35, Loss: 6.605363368988037\n","Epoch: 36, Loss: 6.544915676116943\n","Epoch: 37, Loss: 6.477396011352539\n","Epoch: 38, Loss: 6.415937423706055\n","Epoch: 39, Loss: 6.357247829437256\n","Epoch: 40, Loss: 6.2902374267578125\n","Epoch: 41, Loss: 6.22792911529541\n","Epoch: 42, Loss: 6.1705803871154785\n","Epoch: 43, Loss: 6.106149196624756\n","Epoch: 44, Loss: 6.044586658477783\n","Epoch: 45, Loss: 5.989277362823486\n","Epoch: 46, Loss: 5.928040027618408\n","Epoch: 47, Loss: 5.872622489929199\n","Epoch: 48, Loss: 5.815025329589844\n","Epoch: 49, Loss: 5.7624030113220215\n","Epoch: 50, Loss: 5.702127456665039\n","Epoch: 51, Loss: 5.6479973793029785\n","Epoch: 52, Loss: 5.593760013580322\n","Epoch: 53, Loss: 5.536917209625244\n","Epoch: 54, Loss: 5.48216438293457\n","Epoch: 55, Loss: 5.430120944976807\n","Epoch: 56, Loss: 5.378754615783691\n","Epoch: 57, Loss: 5.321107864379883\n","Epoch: 58, Loss: 5.276313781738281\n","Epoch: 59, Loss: 5.2229905128479\n","Epoch: 60, Loss: 5.170270919799805\n","Epoch: 61, Loss: 5.128371715545654\n","Epoch: 62, Loss: 5.071301460266113\n","Epoch: 63, Loss: 5.021713733673096\n","Epoch: 64, Loss: 4.971399784088135\n","Epoch: 65, Loss: 4.916893005371094\n","Epoch: 66, Loss: 4.875874042510986\n","Epoch: 67, Loss: 4.829169750213623\n","Epoch: 68, Loss: 4.781093597412109\n","Epoch: 69, Loss: 4.728343486785889\n","Epoch: 70, Loss: 4.6806206703186035\n","Epoch: 71, Loss: 4.641161918640137\n","Epoch: 72, Loss: 4.586380958557129\n","Epoch: 73, Loss: 4.535915374755859\n","Epoch: 74, Loss: 4.501369476318359\n","Epoch: 75, Loss: 4.45383882522583\n","Epoch: 76, Loss: 4.402640342712402\n","Epoch: 77, Loss: 4.361020088195801\n","Epoch: 78, Loss: 4.310098648071289\n","Epoch: 79, Loss: 4.271479606628418\n","Epoch: 80, Loss: 4.228338718414307\n","Epoch: 81, Loss: 4.176084041595459\n","Epoch: 82, Loss: 4.137190818786621\n","Epoch: 83, Loss: 4.09087610244751\n","Epoch: 84, Loss: 4.043947696685791\n","Epoch: 85, Loss: 4.000190734863281\n","Epoch: 86, Loss: 3.9561896324157715\n","Epoch: 87, Loss: 3.917875051498413\n","Epoch: 88, Loss: 3.875331401824951\n","Epoch: 89, Loss: 3.8312008380889893\n","Epoch: 90, Loss: 3.788635492324829\n","Epoch: 91, Loss: 3.745105504989624\n","Epoch: 92, Loss: 3.6990158557891846\n","Epoch: 93, Loss: 3.6617283821105957\n","Epoch: 94, Loss: 3.618424654006958\n","Epoch: 95, Loss: 3.574862003326416\n","Epoch: 96, Loss: 3.528266668319702\n","Epoch: 97, Loss: 3.49662709236145\n","Epoch: 98, Loss: 3.4490127563476562\n","Epoch: 99, Loss: 3.4105706214904785\n","Epoch: 100, Loss: 3.374126434326172\n"]}],"source":["criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","\n","transformer.train()\n","\n","for epoch in range(100):\n","    optimizer.zero_grad()\n","    output = transformer(src_data, tgt_data[:, :-1])\n","    loss = criterion(output.contiguous().view(-1, output_lib_size), tgt_data[:, 1:].contiguous().view(-1))\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Pzoj5YTJhvIZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761572002407,"user_tz":-540,"elapsed":8229,"user":{"displayName":"이은지","userId":"07778628580437269752"}},"outputId":"2bbb2b77-8811-46c8-998b-5c13ff163f42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Assignment2-1_Transformer_from_scratch.ipynb\n","tar: Assignment2-1_Transformer_from_scratch.ipynb: file changed as we read it\n","Assignment2-2_ViT.ipynb\n","tar: Assignment2-2_ViT.ipynb: file changed as we read it\n","vit_amp.pth\n","tar: vit_amp.pth: file changed as we read it\n","vit.pth\n","tar: vit.pth: file changed as we read it\n"]}],"source":["# For Google Colab\n","# Uncomment the next line and run. The compressed file will be in your assignment directory.\n","!bash CollectSubmission.sh [2023-18676]"]},{"cell_type":"markdown","metadata":{"id":"vxb1yfpahvIZ"},"source":["### Describe what you did and discovered here\n","In this cell you should write all the settings tried and performances you obtained. Report what you did and what you discovered from the trials.\n","You can write in Korean"]},{"cell_type":"markdown","metadata":{"id":"HLaP1HL5hvIZ"},"source":["1. 포지셔널 인코딩을 구현해서 embedding과 더함\n","2. W_q, W_k 등 파라미터 선언, 스코어 계산, value 계산\n","3. 선형 FFN 인코더 레이어에 추가, LayerNorm, dropout 적용\n","4. 디코더 레이어 어텐션, ffn블럭, layernorm, dropout 추가"]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:py37] *","language":"python","name":"conda-env-py37-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}